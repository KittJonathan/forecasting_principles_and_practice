---
title: "Forecasting Principles and Practice"
format: html
editor: visual
---

# Chapter 1 - Getting started

<https://otexts.com/fpp3/intro.html>

## 1.1 What can be forecast?

<https://otexts.com/fpp3/what-can-be-forecast.html>

Which is easiest to forecast (from easiest at the top to hardest at the bottom):

1.  time of sunrise this day next year
2.  timing of next Halley's comet appearance
3.  maximum temperature tomorrow
4.  daily electricity demand in 3 days time
5.  total sales of drugs in Australian pharmacies next month
6.  Google stock price tomorrow
7.  exchange rate of \$US/AUS next week
8.  Google stock price in 6 months time

Something is easier to forecast if:

1.  we have a good understanding of the factors that contribute to it
2.  there is lots of data available
3.  the future is somewhat similar to the past
4.  the forecasts cannot affect the thing we are trying to forecast

## 1.2 Forecasting, goals and planning

<https://otexts.com/fpp3/planning.html>

**Forecasting** is about predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts.

**Goals** are what you would like to happen. Goals should be linked to forecasts and plans, but this does not always occur. Too often, goals are set without any plan for how to achieve them, and no forecasts for whether they are realistic.

**Planning** is a response to forecasts and goals. Planning involves determining the appropriate actions that are required to make your forecasts match your goals.

**Short-term forecasts** are needed for the scheduling of personnel, production and transportation. As part of the scheduling process, forecasts of demand are often also required.

**Medium-term forecasts** are needed to determine future resource requirements, in order to purchase raw materials, hire personnel, or buy machinery and equipment.

**Long-term forecasts** are used in strategic planning. Such decisions must take account of market opportunities, environmental factors and internal resources.

## 1.3 Determining what to forecast

<https://otexts.com/fpp3/determining-what-to-forecast.html>

-   What should be forecast?

-   Consider the forecasting horizon

-   How frequently are forecasts required?

## 1.4 Forecasting data and methods

<https://otexts.com/fpp3/data-methods.html>

If there are no data available, or if the available data are not relevant to the forecasts, then **qualitative forecasting** must be used.

**Quantitative forecasting** can be applied when two conditions are satisfied:

1.  numerical information about the past is available
2.  it is reasonable to assume that some aspects of the past patterns will continue into the future

We wish to forecast the hourly electricity demand (ED) of a hot region during the summer period. A model with predictor variables might be of the form

$$
ED=f(current\ temp,strength\ of\ economy, population, time\ of\ day, day\ of\ week, error)
$$

The "error" term allows for random variation and the effects of relevant variables that are not included in the model.

We could use a **time series model**, of the form

$$
ED_{t+1}=f(ED_t,ED_{t-1},ED_{t-2},ED_{t-3},...,error)
$$

A third type of model combines the features of the above two models:

$$
ED_{t+1}=f(ED_t,current\ temp,time\ of\ day,day\ of\ week,error)
$$

## 1.5 Some case studies

<https://otexts.com/fpp3/case-studies.html>

## 1.6 The basic steps in a forecasting task

<https://otexts.com/fpp3/basic-steps.html>

Step 1: Problem definition

Step 2: Gathering information

Step 3: Preliminary (exploratory) analysis

Step 4: Choosing and fitting models

Step 5: Using and evaluating a forecasting model

## 1.7 The statistical forecasting perspective

A forecast is an estimate of the probabilities of possible futures.

Statistical forecasting:

-   Thing to be forecast: a random variable, $y_t$

-   Forecast distribution: if $I$ is all observations, then $y_{t|I}$ means "the random variable $y_t$ given what we know in $I$

-   The "point forecast" is the mean (or median) of $y_t|I$

-   The "forecast variance" is $var[y_t|I]$

-   A prediction interval or "interval forecast" is a range of values of $y_t$ with high probability

-   With time series, $y_{t|t-1}=y_t|\{y_1,y_2,...,y_{t-1}\}$

-   $\hat{y}_{T+h|T}=E[y_{T+h}|y_1,...,y_T]$ (an h-step forecast taking account of all observations up to time $T$).

# Chapter 2 - Time series graphics

## 2.1 `tsibble` objects

<https://otexts.com/fpp3/tsibbles.html>

Load the `fpp3` package:

```{r}
library(fpp3)
```

Example of a `tsibble` object:

```{r}
global_economy
```

-   15,150 rows and 9 columns

-   The data are 1 year apart: `[1Y]`

-   There is a key variable, `country`, and 263 countries in the dataset

    -   there are 263 separate time series

    -   the key variable determines the different time series

-   An index variable is the variable that indexes the time series (e.g. `year`)

-   All the other variables are *measured variables*

Another example of `tsibble` object:

```{r}
tourism
```

-   A `tsibble` allows storage and manipulation of multiple time series in R

-   It contains:

    -   An index: time information about the observation

    -   Measured variable(s): numbers of interest

    -   Key variable(s): optional unique identifiers for each series

-   It works with `tidyverse` functions

To create a `tsibble` object:

```{r}
mydata <- tsibble(
  year = 2015:2019,
  y = c(123, 39, 78, 52, 110),
  index = year
)

mydata
```

Converting an existing data frame (or `tibble`) into a `tsibble`:

```{r}
mydata <- tibble(
  year = 2015:2019,
  y = c(123, 39, 78, 52, 110)
  ) |> 
  as_tsibble(index = year)

mydata
```

For observations more frequent than once per year, we need to use a time class function on the index.

```{r}
z <- tibble(
  Month = c("2019 Jan", "2019 Feb", "2019 Mar", "2019 Apr", "2019 May"),
  Observation = c(50, 23, 34, 30, 25)
)

z
```

```{r}
z |> 
  mutate(Month = yearmonth(Month)) |> 
  as_tsibble(index = Month)
```

Common time index variables can be created with these functions:

| Frequency | Function             |
|-----------|----------------------|
| Quarterly | `yearquarter()`      |
| Monthly   | `yearmonth()`        |
| Weekly    | `yearweek()`         |
| Daily     | `as_date()`, `ymd()` |
| Sub-daily | `as_datetime()`      |

**Australian prison population dataset**

Read a csv file and convert to a tsibble

```{r}
prison <- readr::read_csv("data/prison_population.csv")
```

```{r}
prison
```

The observations are in fact quarterly:

```{r}
prison |> 
  distinct(date) |> 
  mutate(month = month(date)) |> 
  distinct(month)
```

```{r}
prison |> 
  mutate(Quarter = yearquarter(date)) |> 
  select(-date) |> 
  as_tsibble(
    index = Quarter,
    key = c(state, gender, legal, indigenous)
  )
```

**Australian Pharmaceutical Benefits Scheme**

```{r}
PBS
```

```{r}
PBS |> 
  filter(ATC2 == "A10") |> 
  select(Month, Concession, Type, Cost) |> 
  summarise(TotalC = sum(Cost))
```

No need to group by month, since this in inherent to the index variable.

```{r}
a10 <- PBS |> 
  filter(ATC2 == "A10") |> 
  select(Month, Concession, Type, Cost) |> 
  summarise(TotalC = sum(Cost)) |> 
  mutate(Cost = TotalC / 1e6)
```

**Olympic running**

```{r}
olympic_running
```

```{r}
olympic_running |> 
  distinct(Sex)
```

**The seasonal period**

| Data     | Minute | Hour | Day   | Week   | Year     |
|----------|--------|------|-------|--------|----------|
| Quarters |        |      |       |        | 4        |
| Months   |        |      |       |        | 12       |
| Weeks    |        |      |       |        | 52       |
| Days     |        |      |       | 7      | 365.25   |
| Hours    |        |      | 24    | 168    | 8766     |
| Minutes  |        | 60   | 1440  | 10080  | 525960   |
| Seconds  | 60     | 3600 | 86400 | 604800 | 31557600 |

## 2.2 Time plots

<https://otexts.com/fpp3/time-plots.html>

Plots allow us to identify:

-   Patterns

-   Unusual observations

-   Changes over time

-   Relationships between variables

```{r}
a10 |> 
  autoplot() +
  labs(y = "$ (millions)",
       title = "Australian antidiabetic drug sales")
```

It will pick the first variable it sees - hence select appropriate variables

Points/observations are joint by lines

```{r}
a10 |> 
  ggplot(aes(x = Month, y = Cost)) +
  geom_point()
```

```{r}
a10 |> 
  ggplot(aes(x = Month, y = Cost)) +
  geom_line()
```

```{r}
a10 |> 
  autoplot(Cost)
```

```{r}
a10 |> 
  autoplot(Cost) +
  geom_point()
```

```{r}
a10 |> 
  autoplot(Cost) +
  labs(title = "Antidiabetic drug sales",
       y = "$ million")
```

```{r}
ansett
```

-   Weekly data

-   4 variables

-   2 keys: Airports, Class

-   1 time index

-   1 passengers

```{r}
ansett |> autoplot(Passengers)
```

```{r}
ansett |> distinct(Class)
```

```{r}
ansett |> distinct(Airports)
```

10 airports \* 3 classes = 30 unique time series

```{r}
ansett |> 
  filter(Class == "Economy") |> 
  autoplot()
```

```{r}
melsyd_economy <- ansett |> 
  filter(Airports == "MEL-SYD") |> 
  select(-Airports)
melsyd_economy
```

```{r}
melsyd_economy |> autoplot()
```

```{r}
melsyd_economy |> 
  filter(Class == "Economy") |> 
  mutate(Passengers = Passengers / 1000) |> 
  autoplot(Passengers) +
  labs(title = "Ansett airlines economy class",
       subtitle = "Melbourne-Sydney",
       y = "Passengers ('000)")
```

## 2.3 Time series patterns

<https://otexts.com/fpp3/tspatterns.html>

-   **Trend**: pattern exists when there is a long-term increase or decrease in the data

-   **Seasonal**: pattern exists when a series is influenced by seasonal factors (e.g. the quarter of the year, the month, or day of the week)

-   **Cyclic**: pattern exists when data exhibit rises and falls that are *not of fixed period* (duration usually of at least 2 years).

Differences between seasonal and cyclic patterns:

-   seasonal pattern constant length; cyclic pattern variable length

-   average length of cycle longer than length of seasonal pattern

-   magnitude of cycle more variable than magnitude of seasonal pattern

The timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.

```{r}
aus_production |> 
  filter(year(Quarter) >= 1980) |> 
  autoplot(Electricity) +
  labs(y = "GWh", title = "Australian electricity production")
```

-   Trend: long term increase

-   Seasonal: regular peaks every 4 observations

```{r}
aus_production |> 
  autoplot(Bricks) +
  labs(y = "million units", title = "Australian clay brick production")
```

-   mid 50s to mid 70s: increase in trend

-   recession early 80s

-   cyclical pattern after the 80s

```{r}
us_employment |> 
  filter(Title == "Retail Trade", year(Month) >= 1980) |> 
  autoplot(Employed / 1e3) +
  labs(y = "Million people", title = "Retail employment, USA")
```

-   Peak in the summer months

-   Cyclical features

```{r}
gafa_stock |> 
  filter(Symbol == "AMZN", year(Date) >= 2018) |> 
  autoplot(Close) +
  labs(y = "$US", title = "Amazon closing stock price")
```

-   Financial data

-   Wandering behaviour (random walk)

```{r}
pelt |> 
  autoplot(Lynx) +
  labs(y = "Number trapped", title = "Annual Canadian Lynx Trappings")
```

-   Annual data does not have seasonality

## 2.4 Seasonal plots

```{r}
a10 |> 
  autoplot(Cost)
```

Seasonal plot: the data are plotted against the individual "seasons" in which the data were observed.

```{r}
a10 |> 
  gg_season(Cost, labels = "both") +
  labs(y = "$ million", 
       title = "Seasonal plot: antidiabetic drug sales")
```

-   Data plotted against the individual "seasons" in which the data were observed (in this case a "season" is a month

-   Something like a time plot except that the data from each season are overlapped

-   Enables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.

-   In R: `gg_season()`

```{r}
beer <- aus_production |> 
  select(Quarter, Beer) |> 
  filter(year(Quarter) >= 1992)

beer |> 
  autoplot(Beer) +
  labs(title = "Australian beer production",
       y = "Megalitres")
```

```{r}
beer |> 
  autoplot(Beer) +
  geom_point() +
  labs(title = "Australian beer production",
       y = "Megalitres")
```

```{r}
beer |> 
  gg_season(Beer, labels = "right")
```

```{r}
vic_elec
```

```{r}
vic_elec |> 
  autoplot()
```

```{r}
vic_elec |> 
  gg_season(Demand)
```

```{r}
vic_elec |> 
  gg_season(Demand, period = "week")
```

```{r}
vic_elec |> 
  gg_season(Demand, period = "day")
```

## 2.5 Seasonal subseries plots

```{r}
a10 |>
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )
```

-   Data for each season collected together in time plot as separate time series

-   Enables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized

-   In R: `gg_subseries()`

```{r}
beer <- aus_production |> 
  select(Quarter, Beer) |> 
  filter(year(Quarter) >= 1992)
 
beer |> autoplot(Beer)
```

```{r}
beer |> gg_subseries(Beer)
```

The global downward trend is due to a decrease in Q4.

```{r}
holidays <- tourism |>
  filter(Purpose == "Holiday") |>
  group_by(State) |>
  summarise(Trips = sum(Trips))
```

```{r}
holidays
```

```{r}
holidays |> 
  autoplot(Trips) +
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")
```

```{r}
holidays |> 
  gg_season(Trips) +
  facet_wrap(vars(State), nrow = 2, scales = "free_y") +
  labs(y = "thousands of trips", 
       title = "Australian domestic holiday nights")
```

```{r}
holidays |> 
  gg_subseries(Trips) +
  labs(y = "thousands of trips",
       title = "Australian domestic holiday nights")
```

## 2.6 Scatter plots

```{r}
vic_elec_day_type <- vic_elec |> 
  filter(year(Time) == 2014) |> 
  mutate(Day_type = case_when(
    Holiday ~ "Holiday",
    wday(Date) %in% 2:6 ~ "Weekday",
    .default = "Weekend"))

vic_elec_day_type
```

```{r}
vic_elec_day_type |> 
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity Demand (GW)")
```

```{r}
vic_elec_day_type |> 
  ggplot(aes(x = Temperature, y = Demand, color = Day_type)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity Demand (GW)")
```

Correlation coefficient: measures the extent of a linear relationship between two variables (y and x)

-   $$
    r=\frac{\sum^T_{t=1}(y_t-\bar{y})(x_t-\bar{x})}{\sqrt{\sum^T_{t=1}(y-\bar{y})^2}\sqrt{\sum^T_{t=1}(x-\bar{x})^2}}
    $$

-   lies between -1 and 1

```{r}
us_change |> 
  GGally::ggpairs(columns = 2:6)
```

```{r}
visitors <- tourism |>
  group_by(State) |>
  summarise(Trips = sum(Trips))

visitors |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")
```

```{r}
visitors |>
  pivot_wider(values_from=Trips, names_from=State) |>
  GGally::ggpairs(columns = 2:9)
```

## 2.7 Lag plots

```{r}
new_production <- aus_production |> 
  filter(year(Quarter) >= 1992)
new_production
```

```{r}
new_production |> 
  gg_lag(Beer, geom = "point")
```

Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of $k$.

```{r}
new_production |> 
  gg_lag(Beer)
```

## 2.8 Autocorrelation

```{r}
new_production |> 
  gg_lag(Beer, geom = "point")
```

-   Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of $k$.

-   The autocorrelations are the correlations associated with these scatterplots.

-   $r_1=corr(y_t,y_{t-1})$

-   $r_2=corr(y_t,y_{t-2})$

-   $r_3=corr(y_t,y_{t-3})$

-   ...

We denote the sample autocovariance at lag $k$ by $c_k$ and the sample autocorrelation at lag $k$ by $r_k$. Then define:

$$
c_k=\frac{1}{T}\sum^T_{t=k+1}(y_t-\bar{y})(y_{t-k}-\bar{y})
$$

and

$$
r_k=\frac{c_k}{c_0}
$$

or

$$
r_k=\frac{\sum^T_{t=k+1}(y_t-\bar{y})(y_{t-k}-\bar{y})}{\sum^T_{t=1}(y_t-\bar{y})^2}
$$

-   $r_1$ indicates how successive values of $y$ relate to each other

-   $r_2$ indicates how $y$ values two periods apart relate to each other

-   $r_k$ is almost the same as the samle correlation between $y$ and $y-k$

```{r}
new_production |> 
  ACF(Beer, lag_max = 9)
```

```{r}
new_production |> 
  ACF(Beer, lag_max = 9) |> 
  autoplot()
```

-   Together, the autocorrelations at lags 1, 2, ..., make up the *autocorrelation* or ACF.

-   The plot is known as a **correlogram**

```{r}
new_production |> 
  ACF(Beer) |> 
  autoplot()
```

-   $r_4$ is higher than for the other lags due to **the seasonal pattern in the data**: peaks tend to be **4 quarters** apart and troughs tend to be **4 quarters** apart.

-   $r_2$ is more negative than for the other lags because troughs tend to be 2 quarters behind peaks.

-   When data have a trend, the autocorrelations for small lags tend to be large and positive (because observations nearby in time are also nearby in value)

-   When data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e. at multiples of the seasonal frequency)

-   When data are trended and seasonal, you see a combination of these effects

```{r}
retail <- us_employment |> 
  filter(Title == "Retail Trade", year(Month) >= 1980)
retail |> autoplot(Employed)
```

Trend + seasonal

```{r}
retail |> 
  ACF(Employed, lag_max = 48) |> 
  autoplot()
```

```{r}
google_2015 <- gafa_stock |> 
  filter(Symbol == "GOOG", year(Date) == 2015) |> 
  select(Date, Close)
google_2015
```

```{r}
google_2015 |> autoplot(Close)
```

```{r}
google_2015 |> 
  ACF(Close, lag_max = 100)
```

```{r}
google_2015 |> 
  ACF(Close, lag_max = 100) |> 
  autoplot()
```

## 2.9 White noise

White noise contains all the frequencies in the spectrum.

```{r}
set.seed(30)
wn <- tsibble(t = 1:50, y = rnorm(50), index = t)
wn |> autoplot()
```

White noise data is uncorrelated across time with zero mean and constant variance.

(Technically, we require independence as well).

```{r}
wn |> ACF(y = ,lag_max = 10)
```

```{r}
wn |> ACF(y) |> autoplot()
```

-   Sample autocorrelations for white noise series

-   Expect each autocorrelation to be close to zero

-   Blue lines show 95% critical values

Sampling distribution of $r_k$ for white noise data is asymptotically $N(0,1/T)$

-   95% of all $r_k$ for white noise must lie within $\pm1.96/\sqrt{T}$

-   If this is not the case, the series is probably not white noise

-   Common to plot lines at $\pm1.96/\sqrt{T}$ when plotting ACF. These are the **critical values**

```{r}
pigs <- aus_livestock |> 
  filter(State == "Victoria", Animal == "Pigs", year(Month) >= 2014)

pigs |> autoplot(Count / 1e3) +
  labs(y = "Thousands", title = "Number of pigs slaughtered in Victoria")
```

```{r}
pigs |> 
  ACF(Count) |> 
  autoplot()
```

-   Difficult to detect pattern in time plot

-   ACF shows significant autocorrelation for lags 2 and 12

-   Indicates some slight seasonality

These show the series is not a white noise series.

## 2.10 Exercises

## 2.11 Further reading

# Chapter 3 - Time series decomposition

3 types of time series patterns: trend, seasonality and cycles.

When we decompose a time series into components, we usually combine the trend and cycle into a single **trend-cycle** component (often just called the **trend** for simplicity). Thus we can think of a time series as comprising three components:

-   a trend-cycle component

-   a seasonal component

-   a remainder component

For some time series (e.g. those that are observed at least daily), there can be more than one seasonal component, corresponding to the different seasonal periods.

## 3.1 Transformations and adjustments

**Par capita adjustments**

```{r}
global_economy |> 
  filter(Country == "Australia") |> 
  autoplot(GDP)
```

```{r}
global_economy |> 
  filter(Country == "Australia") |> 
  autoplot(GDP / Population)
```

**Inflation adjustments**

```{r}
print_retail <- aus_retail |> 
  filter(Industry == "Newspaper and book retailing") |> 
  group_by(Industry) |> 
  index_by(Year =  year(Month)) |> 
  summarise(Turnover = sum(Turnover))

aus_economy <- global_economy |> 
  filter(Code == "AUS")

print_retail |> 
  left_join(aus_economy, by = "Year") |> 
  mutate(Adjusted_turnover = Turnover / CPI * 100) |> 
  pivot_longer(c(Turnover, Adjusted_turnover),
               values_to = "Turnover") |> 
  mutate(name = factor(name, levels = c("Turnover", "Adjusted_turnover"))) |> 
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry", y = "$AU")
```

**Mathematical transformations**

If the data show different variations at different levels of the series, then a transformation can be useful.

Denote original observations as $y_1, ..., y_T$ and transformed observations as $w_1, ..., w_T$.

|             |                     |            |
|-------------|---------------------|------------|
| Square root | $w_t=\sqrt{y_t}$    | ⬊          |
| Cube root   | $w_t=\sqrt[3]{y_t}$ | increasing |
| Logarithm   | $w_t=log(y_t)$      | strength   |

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

```{r}
food <- aus_retail |> 
  filter(Industry == "Food retailing") |> 
  summarise(Turnover = sum(Turnover))
```

```{r}
food |> 
  autoplot(Turnover) +
  labs(y = "Turnover ($AUD)")
```

```{r}
food |> 
  autoplot(sqrt(Turnover)) +
  labs(y = "Square root turnover")
```

```{r}
food |> 
  autoplot(Turnover^(1/3)) +
  labs(y = "Cube root turnover")
```

```{r}
food |> 
  autoplot(log(Turnover)) +
  labs(y = "Log turnover")
```

```{r}
food |> 
  autoplot(-1 / Turnover) +
  labs(y = "Inverse turnover")
```

Each of these transformations is close to a member of the family of **Box-Cox transformations**:

$$
w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (sign(y_t)|y_t|^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

Actually the Bickel-Doksum transformation (allowing for $y_t<0$:

-   $\lambda=1$: no substantive transformation

-   $\lambda=\frac{1}{2}$: square root plus linear transformation

-   $\lambda=0$: natural logarithm

-   $\lambda=-1$: inverse plus 1

**Box-Cox transformations**

```{r}
food |> 
  features(Turnover, features = guerrero)
```

-   This attempts to balance the seasonal fluctuations and random variations across the series

-   Always check the results

-   A low value of $\lambda$ can give extremely large prediction intervals

```{r}
food |> 
  autoplot(box_cox(Turnover, 0.0524)) +
  labs(y = "Box-Cox transformed turnover")
```

-   Often no transformation needed

-   Simple transformations are easier to explain and work well enough

-   Transformations can have very large effect on PI

-   If some data are zero or negative, then use $\lambda>0$

-   `log1p()` can also be useful for data with zeros

-   Choosing logs is a simple way to force forecasts to be positive

-   Transformations must be reversed to obtain forecasts on the original scale (handled automatically by `fable`).

```{r}
lambda <- aus_production |> 
  features(Gas, features = guerrero) |> 
  pull(lambda_guerrero)

aus_production |> 
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda, 2)
       )))
```

## 3.2 Time series components

$$
y_t=f(S_t,T_t,R_t)
$$

where:

$y_t$ = data at period $t$

$T_t$ = trend-cycle component at period $t$

$S_t$ = seasonal component at period $t$

$R_t$ = remainder component at period $t$

**Additive decomposition**: $y_t=S_t+T_t+R_t$

**Multiplicative decomposition**: $y_t=S_t \times T_t \times R_t$

-   Additive model appropriate if magnitude of seasonal fluctuations does not vary with level

-   If seasonal are proportional to level of series, then multiplicative model appropriate

-   Multiplicative decomposition more prevalent with economic series

-   Alternative: use a Box-Cox transformation, and then use additive decomposition

-   Logs turn multiplicative relationship into an additive relationship

$y_t=S_t \times T_t \times R_t$ –\> $log(y_t)=log(S_t)+log(T_t)+log(R_t)$

```{r}
us_retail_employment <- us_employment |> 
  filter(year(Month) >= 1990, Title == "Retail Trade") |> 
  select(-Series_ID)
us_retail_employment
```

```{r}
us_retail_employment |> 
  autoplot(Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

```{r}
us_retail_employment |> 
  model(stl = STL(Employed))
```

```{r}
dcmp <- us_retail_employment |> 
  model(stl = STL(Employed))
components(dcmp)
```

$$
season\_adjust=y_t-S_t
$$

```{r}
components(dcmp) |> autoplot()
```

```{r}
us_retail_employment |> 
  autoplot(Employed, color = "gray") +
  autolayer(components(dcmp), trend, color = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

```{r}
components(dcmp) |> gg_subseries(season_year)
```

-   Useful by-product of decomposition: an easy way to calculate seasonally adjusted data

-   Additive decomposition: seasonally adjusted data given by

$$
y_t-S_t=T_t+R_t
$$

-   Multiplicative decomposition: seasonally adjusted data given by

$$
y_t/S_t=T_t \times R_t
$$

```{r}
us_retail_employment |> 
  autoplot(Employed, color = "gray") +
  autolayer(components(dcmp), season_adjust, color = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

-   We use estimates of $S$ based on past values to seasonally adjust a current value

-   Seasonally adjusted series reflect **remainders** as well as **trend**. Therefore they are not "smooth" and "downturns" or "upturns" can be misleading

-   It is better to use the trend-cycle component to look for turning points
