---
title: "Forecasting Principles and Practice"
format: html
editor: visual
---

# Chapter 1 - Getting started

<https://otexts.com/fpp3/intro.html>

## 1.1 What can be forecast?

<https://otexts.com/fpp3/what-can-be-forecast.html>

Which is easiest to forecast (from easiest at the top to hardest at the bottom):

1.  time of sunrise this day next year
2.  timing of next Halley's comet appearance
3.  maximum temperature tomorrow
4.  daily electricity demand in 3 days time
5.  total sales of drugs in Australian pharmacies next month
6.  Google stock price tomorrow
7.  exchange rate of \$US/AUS next week
8.  Google stock price in 6 months time

Something is easier to forecast if:

1.  we have a good understanding of the factors that contribute to it
2.  there is lots of data available
3.  the future is somewhat similar to the past
4.  the forecasts cannot affect the thing we are trying to forecast

## 1.2 Forecasting, goals and planning

<https://otexts.com/fpp3/planning.html>

**Forecasting** is about predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts.

**Goals** are what you would like to happen. Goals should be linked to forecasts and plans, but this does not always occur. Too often, goals are set without any plan for how to achieve them, and no forecasts for whether they are realistic.

**Planning** is a response to forecasts and goals. Planning involves determining the appropriate actions that are required to make your forecasts match your goals.

**Short-term forecasts** are needed for the scheduling of personnel, production and transportation. As part of the scheduling process, forecasts of demand are often also required.

**Medium-term forecasts** are needed to determine future resource requirements, in order to purchase raw materials, hire personnel, or buy machinery and equipment.

**Long-term forecasts** are used in strategic planning. Such decisions must take account of market opportunities, environmental factors and internal resources.

## 1.3 Determining what to forecast

<https://otexts.com/fpp3/determining-what-to-forecast.html>

-   What should be forecast?

-   Consider the forecasting horizon

-   How frequently are forecasts required?

## 1.4 Forecasting data and methods

<https://otexts.com/fpp3/data-methods.html>

If there are no data available, or if the available data are not relevant to the forecasts, then **qualitative forecasting** must be used.

**Quantitative forecasting** can be applied when two conditions are satisfied:

1.  numerical information about the past is available
2.  it is reasonable to assume that some aspects of the past patterns will continue into the future

We wish to forecast the hourly electricity demand (ED) of a hot region during the summer period. A model with predictor variables might be of the form

$$
ED=f(current\ temp,strength\ of\ economy, population, time\ of\ day, day\ of\ week, error)
$$

The "error" term allows for random variation and the effects of relevant variables that are not included in the model.

We could use a **time series model**, of the form

$$
ED_{t+1}=f(ED_t,ED_{t-1},ED_{t-2},ED_{t-3},...,error)
$$

A third type of model combines the features of the above two models:

$$
ED_{t+1}=f(ED_t,current\ temp,time\ of\ day,day\ of\ week,error)
$$

## 1.5 Some case studies

<https://otexts.com/fpp3/case-studies.html>

## 1.6 The basic steps in a forecasting task

<https://otexts.com/fpp3/basic-steps.html>

Step 1: Problem definition

Step 2: Gathering information

Step 3: Preliminary (exploratory) analysis

Step 4: Choosing and fitting models

Step 5: Using and evaluating a forecasting model

## 1.7 The statistical forecasting perspective

A forecast is an estimate of the probabilities of possible futures.

Statistical forecasting:

-   Thing to be forecast: a random variable, $y_t$

-   Forecast distribution: if $I$ is all observations, then $y_{t|I}$ means "the random variable $y_t$ given what we know in $I$

-   The "point forecast" is the mean (or median) of $y_t|I$

-   The "forecast variance" is $var[y_t|I]$

-   A prediction interval or "interval forecast" is a range of values of $y_t$ with high probability

-   With time series, $y_{t|t-1}=y_t|\{y_1,y_2,...,y_{t-1}\}$

-   $\hat{y}_{T+h|T}=E[y_{T+h}|y_1,...,y_T]$ (an h-step forecast taking account of all observations up to time $T$).

# Chapter 2 - Time series graphics

## 2.1 `tsibble` objects

<https://otexts.com/fpp3/tsibbles.html>

Load the `fpp3` package:

```{r}
library(fpp3)
```

Example of a `tsibble` object:

```{r}
global_economy
```

-   15,150 rows and 9 columns

-   The data are 1 year apart: `[1Y]`

-   There is a key variable, `country`, and 263 countries in the dataset

    -   there are 263 separate time series

    -   the key variable determines the different time series

-   An index variable is the variable that indexes the time series (e.g. `year`)

-   All the other variables are *measured variables*

Another example of `tsibble` object:

```{r}
tourism
```

-   A `tsibble` allows storage and manipulation of multiple time series in R

-   It contains:

    -   An index: time information about the observation

    -   Measured variable(s): numbers of interest

    -   Key variable(s): optional unique identifiers for each series

-   It works with `tidyverse` functions

To create a `tsibble` object:

```{r}
mydata <- tsibble(
  year = 2015:2019,
  y = c(123, 39, 78, 52, 110),
  index = year
)

mydata
```

Converting an existing data frame (or `tibble`) into a `tsibble`:

```{r}
mydata <- tibble(
  year = 2015:2019,
  y = c(123, 39, 78, 52, 110)
  ) |> 
  as_tsibble(index = year)

mydata
```

For observations more frequent than once per year, we need to use a time class function on the index.

```{r}
z <- tibble(
  Month = c("2019 Jan", "2019 Feb", "2019 Mar", "2019 Apr", "2019 May"),
  Observation = c(50, 23, 34, 30, 25)
)

z
```

```{r}
z |> 
  mutate(Month = yearmonth(Month)) |> 
  as_tsibble(index = Month)
```

Common time index variables can be created with these functions:

| Frequency | Function             |
|-----------|----------------------|
| Quarterly | `yearquarter()`      |
| Monthly   | `yearmonth()`        |
| Weekly    | `yearweek()`         |
| Daily     | `as_date()`, `ymd()` |
| Sub-daily | `as_datetime()`      |

**Australian prison population dataset**

Read a csv file and convert to a tsibble

```{r}
prison <- readr::read_csv("data/prison_population.csv")
```

```{r}
prison
```

The observations are in fact quarterly:

```{r}
prison |> 
  distinct(date) |> 
  mutate(month = month(date)) |> 
  distinct(month)
```

```{r}
prison |> 
  mutate(Quarter = yearquarter(date)) |> 
  select(-date) |> 
  as_tsibble(
    index = Quarter,
    key = c(state, gender, legal, indigenous)
  )
```

**Australian Pharmaceutical Benefits Scheme**

```{r}
PBS
```

```{r}
PBS |> 
  filter(ATC2 == "A10") |> 
  select(Month, Concession, Type, Cost) |> 
  summarise(TotalC = sum(Cost))
```

No need to group by month, since this in inherent to the index variable.

```{r}
a10 <- PBS |> 
  filter(ATC2 == "A10") |> 
  select(Month, Concession, Type, Cost) |> 
  summarise(TotalC = sum(Cost)) |> 
  mutate(Cost = TotalC / 1e6)
```

**Olympic running**

```{r}
olympic_running
```

```{r}
olympic_running |> 
  distinct(Sex)
```

**The seasonal period**

| Data     | Minute | Hour | Day   | Week   | Year     |
|----------|--------|------|-------|--------|----------|
| Quarters |        |      |       |        | 4        |
| Months   |        |      |       |        | 12       |
| Weeks    |        |      |       |        | 52       |
| Days     |        |      |       | 7      | 365.25   |
| Hours    |        |      | 24    | 168    | 8766     |
| Minutes  |        | 60   | 1440  | 10080  | 525960   |
| Seconds  | 60     | 3600 | 86400 | 604800 | 31557600 |

## 2.2 Time plots

<https://otexts.com/fpp3/time-plots.html>

Plots allow us to identify:

-   Patterns

-   Unusual observations

-   Changes over time

-   Relationships between variables

```{r}
a10 |> 
  autoplot() +
  labs(y = "$ (millions)",
       title = "Australian antidiabetic drug sales")
```

It will pick the first variable it sees - hence select appropriate variables

Points/observations are joint by lines

```{r}
a10 |> 
  ggplot(aes(x = Month, y = Cost)) +
  geom_point()
```

```{r}
a10 |> 
  ggplot(aes(x = Month, y = Cost)) +
  geom_line()
```

```{r}
a10 |> 
  autoplot(Cost)
```

```{r}
a10 |> 
  autoplot(Cost) +
  geom_point()
```

```{r}
a10 |> 
  autoplot(Cost) +
  labs(title = "Antidiabetic drug sales",
       y = "$ million")
```

```{r}
ansett
```

-   Weekly data

-   4 variables

-   2 keys: Airports, Class

-   1 time index

-   1 passengers

```{r}
ansett |> autoplot(Passengers)
```

```{r}
ansett |> distinct(Class)
```

```{r}
ansett |> distinct(Airports)
```

10 airports \* 3 classes = 30 unique time series

```{r}
ansett |> 
  filter(Class == "Economy") |> 
  autoplot()
```

```{r}
melsyd_economy <- ansett |> 
  filter(Airports == "MEL-SYD") |> 
  select(-Airports)
melsyd_economy
```

```{r}
melsyd_economy |> autoplot()
```

```{r}
melsyd_economy |> 
  filter(Class == "Economy") |> 
  mutate(Passengers = Passengers / 1000) |> 
  autoplot(Passengers) +
  labs(title = "Ansett airlines economy class",
       subtitle = "Melbourne-Sydney",
       y = "Passengers ('000)")
```

## 2.3 Time series patterns

<https://otexts.com/fpp3/tspatterns.html>

-   **Trend**: pattern exists when there is a long-term increase or decrease in the data

-   **Seasonal**: pattern exists when a series is influenced by seasonal factors (e.g. the quarter of the year, the month, or day of the week)

-   **Cyclic**: pattern exists when data exhibit rises and falls that are *not of fixed period* (duration usually of at least 2 years).

Differences between seasonal and cyclic patterns:

-   seasonal pattern constant length; cyclic pattern variable length

-   average length of cycle longer than length of seasonal pattern

-   magnitude of cycle more variable than magnitude of seasonal pattern

The timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data.

```{r}
aus_production |> 
  filter(year(Quarter) >= 1980) |> 
  autoplot(Electricity) +
  labs(y = "GWh", title = "Australian electricity production")
```

-   Trend: long term increase

-   Seasonal: regular peaks every 4 observations

```{r}
aus_production |> 
  autoplot(Bricks) +
  labs(y = "million units", title = "Australian clay brick production")
```

-   mid 50s to mid 70s: increase in trend

-   recession early 80s

-   cyclical pattern after the 80s

```{r}
us_employment |> 
  filter(Title == "Retail Trade", year(Month) >= 1980) |> 
  autoplot(Employed / 1e3) +
  labs(y = "Million people", title = "Retail employment, USA")
```

-   Peak in the summer months

-   Cyclical features

```{r}
gafa_stock |> 
  filter(Symbol == "AMZN", year(Date) >= 2018) |> 
  autoplot(Close) +
  labs(y = "$US", title = "Amazon closing stock price")
```

-   Financial data

-   Wandering behaviour (random walk)

```{r}
pelt |> 
  autoplot(Lynx) +
  labs(y = "Number trapped", title = "Annual Canadian Lynx Trappings")
```

-   Annual data does not have seasonality

## 2.4 Seasonal plots

```{r}
a10 |> 
  autoplot(Cost)
```

Seasonal plot: the data are plotted against the individual "seasons" in which the data were observed.

```{r}
a10 |> 
  gg_season(Cost, labels = "both") +
  labs(y = "$ million", 
       title = "Seasonal plot: antidiabetic drug sales")
```

-   Data plotted against the individual "seasons" in which the data were observed (in this case a "season" is a month

-   Something like a time plot except that the data from each season are overlapped

-   Enables the underlying seasonal pattern to be seen more clearly, and also allows any substantial departures from the seasonal pattern to be easily identified.

-   In R: `gg_season()`

```{r}
beer <- aus_production |> 
  select(Quarter, Beer) |> 
  filter(year(Quarter) >= 1992)

beer |> 
  autoplot(Beer) +
  labs(title = "Australian beer production",
       y = "Megalitres")
```

```{r}
beer |> 
  autoplot(Beer) +
  geom_point() +
  labs(title = "Australian beer production",
       y = "Megalitres")
```

```{r}
beer |> 
  gg_season(Beer, labels = "right")
```

```{r}
vic_elec
```

```{r}
vic_elec |> 
  autoplot()
```

```{r}
vic_elec |> 
  gg_season(Demand)
```

```{r}
vic_elec |> 
  gg_season(Demand, period = "week")
```

```{r}
vic_elec |> 
  gg_season(Demand, period = "day")
```

## 2.5 Seasonal subseries plots

```{r}
a10 |>
  gg_subseries(Cost) +
  labs(
    y = "$ (millions)",
    title = "Australian antidiabetic drug sales"
  )
```

-   Data for each season collected together in time plot as separate time series

-   Enables the underlying seasonal pattern to be seen clearly, and changes in seasonality over time to be visualized

-   In R: `gg_subseries()`

```{r}
beer <- aus_production |> 
  select(Quarter, Beer) |> 
  filter(year(Quarter) >= 1992)
 
beer |> autoplot(Beer)
```

```{r}
beer |> gg_subseries(Beer)
```

The global downward trend is due to a decrease in Q4.

```{r}
holidays <- tourism |>
  filter(Purpose == "Holiday") |>
  group_by(State) |>
  summarise(Trips = sum(Trips))
```

```{r}
holidays
```

```{r}
holidays |> 
  autoplot(Trips) +
  labs(y = "Overnight trips ('000)",
       title = "Australian domestic holidays")
```

```{r}
holidays |> 
  gg_season(Trips) +
  facet_wrap(vars(State), nrow = 2, scales = "free_y") +
  labs(y = "thousands of trips", 
       title = "Australian domestic holiday nights")
```

```{r}
holidays |> 
  gg_subseries(Trips) +
  labs(y = "thousands of trips",
       title = "Australian domestic holiday nights")
```

## 2.6 Scatter plots

```{r}
vic_elec_day_type <- vic_elec |> 
  filter(year(Time) == 2014) |> 
  mutate(Day_type = case_when(
    Holiday ~ "Holiday",
    wday(Date) %in% 2:6 ~ "Weekday",
    .default = "Weekend"))

vic_elec_day_type
```

```{r}
vic_elec_day_type |> 
  ggplot(aes(x = Temperature, y = Demand)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity Demand (GW)")
```

```{r}
vic_elec_day_type |> 
  ggplot(aes(x = Temperature, y = Demand, color = Day_type)) +
  geom_point() +
  labs(x = "Temperature (degrees Celsius)",
       y = "Electricity Demand (GW)")
```

Correlation coefficient: measures the extent of a linear relationship between two variables (y and x)

-   $$
    r=\frac{\sum^T_{t=1}(y_t-\bar{y})(x_t-\bar{x})}{\sqrt{\sum^T_{t=1}(y-\bar{y})^2}\sqrt{\sum^T_{t=1}(x-\bar{x})^2}}
    $$

-   lies between -1 and 1

```{r}
us_change |> 
  GGally::ggpairs(columns = 2:6)
```

```{r}
visitors <- tourism |>
  group_by(State) |>
  summarise(Trips = sum(Trips))

visitors |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State), scales = "free_y") +
  labs(title = "Australian domestic tourism",
       y= "Overnight trips ('000)")
```

```{r}
visitors |>
  pivot_wider(values_from=Trips, names_from=State) |>
  GGally::ggpairs(columns = 2:9)
```

## 2.7 Lag plots

```{r}
new_production <- aus_production |> 
  filter(year(Quarter) >= 1992)
new_production
```

```{r}
new_production |> 
  gg_lag(Beer, geom = "point")
```

Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of $k$.

```{r}
new_production |> 
  gg_lag(Beer)
```

## 2.8 Autocorrelation

```{r}
new_production |> 
  gg_lag(Beer, geom = "point")
```

-   Each graph shows $y_t$ plotted against $y_{t-k}$ for different values of $k$.

-   The autocorrelations are the correlations associated with these scatterplots.

-   $r_1=corr(y_t,y_{t-1})$

-   $r_2=corr(y_t,y_{t-2})$

-   $r_3=corr(y_t,y_{t-3})$

-   ...

We denote the sample autocovariance at lag $k$ by $c_k$ and the sample autocorrelation at lag $k$ by $r_k$. Then define:

$$
c_k=\frac{1}{T}\sum^T_{t=k+1}(y_t-\bar{y})(y_{t-k}-\bar{y})
$$

and

$$
r_k=\frac{c_k}{c_0}
$$

or

$$
r_k=\frac{\sum^T_{t=k+1}(y_t-\bar{y})(y_{t-k}-\bar{y})}{\sum^T_{t=1}(y_t-\bar{y})^2}
$$

-   $r_1$ indicates how successive values of $y$ relate to each other

-   $r_2$ indicates how $y$ values two periods apart relate to each other

-   $r_k$ is almost the same as the samle correlation between $y$ and $y-k$

```{r}
new_production |> 
  ACF(Beer, lag_max = 9)
```

```{r}
new_production |> 
  ACF(Beer, lag_max = 9) |> 
  autoplot()
```

-   Together, the autocorrelations at lags 1, 2, ..., make up the *autocorrelation* or ACF.

-   The plot is known as a **correlogram**

```{r}
new_production |> 
  ACF(Beer) |> 
  autoplot()
```

-   $r_4$ is higher than for the other lags due to **the seasonal pattern in the data**: peaks tend to be **4 quarters** apart and troughs tend to be **4 quarters** apart.

-   $r_2$ is more negative than for the other lags because troughs tend to be 2 quarters behind peaks.

-   When data have a trend, the autocorrelations for small lags tend to be large and positive (because observations nearby in time are also nearby in value)

-   When data are seasonal, the autocorrelations will be larger at the seasonal lags (i.e. at multiples of the seasonal frequency)

-   When data are trended and seasonal, you see a combination of these effects

```{r}
retail <- us_employment |> 
  filter(Title == "Retail Trade", year(Month) >= 1980)
retail |> autoplot(Employed)
```

Trend + seasonal

```{r}
retail |> 
  ACF(Employed, lag_max = 48) |> 
  autoplot()
```

```{r}
google_2015 <- gafa_stock |> 
  filter(Symbol == "GOOG", year(Date) == 2015) |> 
  select(Date, Close)
google_2015
```

```{r}
google_2015 |> autoplot(Close)
```

```{r}
google_2015 |> 
  ACF(Close, lag_max = 100)
```

```{r}
google_2015 |> 
  ACF(Close, lag_max = 100) |> 
  autoplot()
```

## 2.9 White noise

White noise contains all the frequencies in the spectrum.

```{r}
set.seed(30)
wn <- tsibble(t = 1:50, y = rnorm(50), index = t)
wn |> autoplot()
```

White noise data is uncorrelated across time with zero mean and constant variance.

(Technically, we require independence as well).

```{r}
wn |> ACF(y = ,lag_max = 10)
```

```{r}
wn |> ACF(y) |> autoplot()
```

-   Sample autocorrelations for white noise series

-   Expect each autocorrelation to be close to zero

-   Blue lines show 95% critical values

Sampling distribution of $r_k$ for white noise data is asymptotically $N(0,1/T)$

-   95% of all $r_k$ for white noise must lie within $\pm1.96/\sqrt{T}$

-   If this is not the case, the series is probably not white noise

-   Common to plot lines at $\pm1.96/\sqrt{T}$ when plotting ACF. These are the **critical values**

```{r}
pigs <- aus_livestock |> 
  filter(State == "Victoria", Animal == "Pigs", year(Month) >= 2014)

pigs |> autoplot(Count / 1e3) +
  labs(y = "Thousands", title = "Number of pigs slaughtered in Victoria")
```

```{r}
pigs |> 
  ACF(Count) |> 
  autoplot()
```

-   Difficult to detect pattern in time plot

-   ACF shows significant autocorrelation for lags 2 and 12

-   Indicates some slight seasonality

These show the series is not a white noise series.

## 2.10 Exercises

## 2.11 Further reading

# Chapter 3 - Time series decomposition

3 types of time series patterns: trend, seasonality and cycles.

When we decompose a time series into components, we usually combine the trend and cycle into a single **trend-cycle** component (often just called the **trend** for simplicity). Thus we can think of a time series as comprising three components:

-   a trend-cycle component

-   a seasonal component

-   a remainder component

For some time series (e.g. those that are observed at least daily), there can be more than one seasonal component, corresponding to the different seasonal periods.

## 3.1 Transformations and adjustments

**Par capita adjustments**

```{r}
global_economy |> 
  filter(Country == "Australia") |> 
  autoplot(GDP)
```

```{r}
global_economy |> 
  filter(Country == "Australia") |> 
  autoplot(GDP / Population)
```

**Inflation adjustments**

```{r}
print_retail <- aus_retail |> 
  filter(Industry == "Newspaper and book retailing") |> 
  group_by(Industry) |> 
  index_by(Year =  year(Month)) |> 
  summarise(Turnover = sum(Turnover))

aus_economy <- global_economy |> 
  filter(Code == "AUS")

print_retail |> 
  left_join(aus_economy, by = "Year") |> 
  mutate(Adjusted_turnover = Turnover / CPI * 100) |> 
  pivot_longer(c(Turnover, Adjusted_turnover),
               values_to = "Turnover") |> 
  mutate(name = factor(name, levels = c("Turnover", "Adjusted_turnover"))) |> 
  ggplot(aes(x = Year, y = Turnover)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  labs(title = "Turnover: Australian print media industry", y = "$AU")
```

**Mathematical transformations**

If the data show different variations at different levels of the series, then a transformation can be useful.

Denote original observations as $y_1, ..., y_T$ and transformed observations as $w_1, ..., w_T$.

|             |                     |            |
|-------------|---------------------|------------|
| Square root | $w_t=\sqrt{y_t}$    | ⬊          |
| Cube root   | $w_t=\sqrt[3]{y_t}$ | increasing |
| Logarithm   | $w_t=log(y_t)$      | strength   |

Logarithms, in particular, are useful because they are more interpretable: changes in a log value are **relative (percent) changes on the original scale**.

```{r}
food <- aus_retail |> 
  filter(Industry == "Food retailing") |> 
  summarise(Turnover = sum(Turnover))
```

```{r}
food |> 
  autoplot(Turnover) +
  labs(y = "Turnover ($AUD)")
```

```{r}
food |> 
  autoplot(sqrt(Turnover)) +
  labs(y = "Square root turnover")
```

```{r}
food |> 
  autoplot(Turnover^(1/3)) +
  labs(y = "Cube root turnover")
```

```{r}
food |> 
  autoplot(log(Turnover)) +
  labs(y = "Log turnover")
```

```{r}
food |> 
  autoplot(-1 / Turnover) +
  labs(y = "Inverse turnover")
```

Each of these transformations is close to a member of the family of **Box-Cox transformations**:

$$
w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (sign(y_t)|y_t|^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

Actually the Bickel-Doksum transformation (allowing for $y_t<0$:

-   $\lambda=1$: no substantive transformation

-   $\lambda=\frac{1}{2}$: square root plus linear transformation

-   $\lambda=0$: natural logarithm

-   $\lambda=-1$: inverse plus 1

**Box-Cox transformations**

```{r}
food |> 
  features(Turnover, features = guerrero)
```

-   This attempts to balance the seasonal fluctuations and random variations across the series

-   Always check the results

-   A low value of $\lambda$ can give extremely large prediction intervals

```{r}
food |> 
  autoplot(box_cox(Turnover, 0.0524)) +
  labs(y = "Box-Cox transformed turnover")
```

-   Often no transformation needed

-   Simple transformations are easier to explain and work well enough

-   Transformations can have very large effect on PI

-   If some data are zero or negative, then use $\lambda>0$

-   `log1p()` can also be useful for data with zeros

-   Choosing logs is a simple way to force forecasts to be positive

-   Transformations must be reversed to obtain forecasts on the original scale (handled automatically by `fable`).

```{r}
lambda <- aus_production |> 
  features(Gas, features = guerrero) |> 
  pull(lambda_guerrero)

aus_production |> 
  autoplot(box_cox(Gas, lambda)) +
  labs(y = "",
       title = latex2exp::TeX(paste0(
         "Transformed gas production with $\\lambda$ = ",
         round(lambda, 2)
       )))
```

## 3.2 Time series components

$$
y_t=f(S_t,T_t,R_t)
$$

where:

$y_t$ = data at period $t$

$T_t$ = trend-cycle component at period $t$

$S_t$ = seasonal component at period $t$

$R_t$ = remainder component at period $t$

**Additive decomposition**: $y_t=S_t+T_t+R_t$

**Multiplicative decomposition**: $y_t=S_t \times T_t \times R_t$

-   Additive model appropriate if magnitude of seasonal fluctuations does not vary with level

-   If seasonal are proportional to level of series, then multiplicative model appropriate

-   Multiplicative decomposition more prevalent with economic series

-   Alternative: use a Box-Cox transformation, and then use additive decomposition

-   Logs turn multiplicative relationship into an additive relationship

$y_t=S_t \times T_t \times R_t$ –\> $log(y_t)=log(S_t)+log(T_t)+log(R_t)$

```{r}
us_retail_employment <- us_employment |> 
  filter(year(Month) >= 1990, Title == "Retail Trade") |> 
  select(-Series_ID)
us_retail_employment
```

```{r}
us_retail_employment |> 
  autoplot(Employed) +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

```{r}
us_retail_employment |> 
  model(stl = STL(Employed))
```

```{r}
dcmp <- us_retail_employment |> 
  model(stl = STL(Employed))
components(dcmp)
```

$$
season\_adjust=y_t-S_t
$$

```{r}
components(dcmp) |> autoplot()
```

```{r}
us_retail_employment |> 
  autoplot(Employed, color = "gray") +
  autolayer(components(dcmp), trend, color = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

```{r}
components(dcmp) |> gg_subseries(season_year)
```

-   Useful by-product of decomposition: an easy way to calculate seasonally adjusted data

-   Additive decomposition: seasonally adjusted data given by

$$
y_t-S_t=T_t+R_t
$$

-   Multiplicative decomposition: seasonally adjusted data given by

$$
y_t/S_t=T_t \times R_t
$$

```{r}
us_retail_employment |> 
  autoplot(Employed, color = "gray") +
  autolayer(components(dcmp), season_adjust, color = "#0072B2") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

-   We use estimates of $S$ based on past values to seasonally adjust a current value

-   Seasonally adjusted series reflect **remainders** as well as **trend**. Therefore they are not "smooth" and "downturns" or "upturns" can be misleading

-   It is better to use the trend-cycle component to look for turning points

## 3.3 Moving averages

The simplest estimate of the trend-cycle uses **moving averages**

$$
\hat{T}_t = \frac{1}{m} \sum_{j=-k}^k y_{t+j}
$$

where

$$
k=\frac{m-1}{2}
$$

Example with $m=7$:

$$
k=\frac{m-1}{2}=\frac{7-1}{2}=3
$$

7-MA (Month Average):

$$
\hat{T}_t = \frac{1}{7} \sum_{j=-3}^3 y_{t+j}
$$

$$
\hat{T}_t=\frac{1}{7}(y_{t-3}+y_{t-2}+y_{t-1}+y_t+y_{t+1}+y_{t+2}+y_{t+3})
$$

```{r}
global_economy |> 
  filter(Country == "Australia") |> 
  autoplot(Exports) +
  labs(y = "% of GDP",
       title = "Total Australian Exports")
```

```{r}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `5-MA` = slider::slide_dbl(
      Exports, mean,
      .before = 2, .after = 2, .complete = TRUE)
  )
```

```{r}
aus_exports |> 
  select(Country, Year, Exports, `5-MA`) |> 
  slice(1:5, 54:58)
```

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports: 5-MA")
```

```{r}
aus_exports <- global_economy |>
  filter(Country == "Australia") |>
  mutate(
    `3-MA` = slider::slide_dbl(
      Exports, mean,
      .before = 1, .after = 1, .complete = TRUE)
  ) |> 
  mutate(
    `5-MA` = slider::slide_dbl(
      Exports, mean,
      .before = 2, .after = 2, .complete = TRUE)
  ) |> 
  mutate(
    `7-MA` = slider::slide_dbl(
      Exports, mean,
      .before = 3, .after = 3, .complete = TRUE)
  ) |> 
  mutate(
    `9-MA` = slider::slide_dbl(
      Exports, mean,
      .before = 4, .after = 4, .complete = TRUE)
  ) 
```

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `3-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports: 3-MA")
```

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports: 5-MA")
```

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `7-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports: 7-MA")
```

```{r}
aus_exports |>
  autoplot(Exports) +
  geom_line(aes(y = `9-MA`), colour = "#D55E00") +
  labs(y = "% of GDP",
       title = "Total Australian exports: 9-MA")
```

A moving average is a an **average of nearby points**:

-   observations nearby in time are also likely to be **close in value**

-   average eliminates some **randomness** in the data, leaving a smooth trend-cycle component

-   each average computed by dropping **oldest** observation and including **next** observation

-   averages **moves** through time series until trend-cycle computed at each observation possible

Why is there no estimates at end?

-   For a 3-MA, there cannot be estimates at time 1 or time T because the observations at time 0 and T+1 are not available

-   Generally: there cannot be estimates at times near the endpoints

The order of the MA

-   larger order means smoother, flatter curve

-   larger order means more points lost at ends

-   **order = length of season** *or* cycle removes pattern

-   but so far odd orders?

For even orders:

-   4-MA:

$$
\frac{1}{4}(y_{t-2}+y_{t-1}+y_t+y_{t+1})
$$

or

$$
\frac{1}{4}(y_{t-1}+y_t+y_{t+1}+y_{t+2})
$$

In this case, the data aren't centered. Solution: take a further 2-MA to "centre" the result.

$$
\hat{T}_t=\frac{1}{2}\{\frac{1}{4}(y_{t-2}+y_{t-1}+y_t+y_{t+1})+\frac{1}{4}(y_{t-1}+y_t+y_{t+1}+y_{t+2})\}=\frac{1}{8}y_{t-2}+\frac{1}{4}y_{t-1}+\frac{1}{4}y_t+\frac{1}{4}y_{t+1}+\frac{1}{8}y_{t+2}
$$

A moving average of the same length as the season removes the seasonal pattern:

-   For quarterly data, use a $2 \times 4\ MA$

-   For monthly data, use a $2 \times 12\ MA$

```{r}
beer <- aus_production |>
  filter(year(Quarter) >= 1992) |>
  select(Quarter, Beer)
beer_ma <- beer |>
  mutate(
    `4-MA` = slider::slide_dbl(Beer, mean,
                .before = 1, .after = 2, .complete = TRUE),
    `2x4-MA` = slider::slide_dbl(`4-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
```

```{r}
us_retail_employment_ma <- us_retail_employment |>
  mutate(
    `12-MA` = slider::slide_dbl(Employed, mean,
                .before = 5, .after = 6, .complete = TRUE),
    `2x12-MA` = slider::slide_dbl(`12-MA`, mean,
                .before = 1, .after = 0, .complete = TRUE)
  )
us_retail_employment_ma |>
  autoplot(Employed, colour = "gray") +
  geom_line(aes(y = `2x12-MA`), colour = "#D55E00") +
  labs(y = "Persons (thousands)",
       title = "Total employment in US retail")
```

## 3.4 Classical decomposition

Additive decomposition: $y_t=T_t+S_t+R_t=\hat{T_t}+\hat{S_t}+\hat{R_t}$

Multiplicative decomposition: $y_t=T_t \times S_t \times R_t=\hat{T_t} \times \hat{S_t}_ \times \hat{R_t}$

**1) Step 1**

Estimate $\hat{T}$ using $(2 \times m)-MA$ if $m$ is even. Otherwise, estimate $\hat{T}$ using $m-MA$

**2) Step 2**

Compute de-trended series

-   Additive decomposition: $y_t-\hat{T_t}$

-   Multiplicative decomposition: $y_t / \hat{T_t}$

De-trending

Remove smoothed series $\hat{T_t}$ from $y_t$ to leave $S_t$ and $R_t$.

-   Additive model: $y_t-\hat{T_t}=(\hat{T_t}+\hat{S_t}+\hat{R_t})-\hat{T_t}=\hat{S_t}+\hat{R_t}$

-   Multiplicative model: $y_t/\hat{T_t}=(\hat{T_t} \times \hat{S_t} \times \hat{R_t})/\hat{T_t}=\hat{S_t} \times \hat{R_t}$

**3) Step 3**

Estimating the seasonal component:

-   seasonal index for each season is estimated as an **average** of the detrended series for that season of successive years.

-   e.g., take averages across all Januaries to get $S^{(1)}$ if your data is monthly.

-   If necessary, adjust the seasonal indices so that:

    -   for additive: $S^{(1)} + S^{(2)} + ... + S^{(12)}=0$

    -   for multiplicative: $S^{(1)} + S^{(2)} + ... + S^{(12)}=m$

-   The seasonal component $\hat{S_t}$ simply consists of replications of the seasonal indices.

**4) Step 4**

Remainder component:

-   Additive decomposition: $\hat{R_t}=y_t-\hat{T_t}-\hat{S_t}$

-   Multiplicative decomposition: $\hat{R_t}=y_t/(\hat{T_t}\hat{S_t})$

Classical decomposition:

-   Choose additive or multiplicative depending on which gives the most stable components

-   For multiplicative models, this method of estimation is known as **ratio-to-moving-average method**.

```{r}
us_retail_employment |> 
  model(
    classical_decomposition(Employed, type = "additive")
  ) |> 
  components() |> 
  autoplot() +
  labs(title = "Classical additive decomposition of total US retail employment", x = "Year")
```

```{r}
us_retail_employment |> 
  model(
    classical_decomposition(Employed, type = "multiplicative")
  ) |> 
  components() |> 
  autoplot() +
  labs(title = "Classical multiplicative decomposition of total US retail employment", x = "Year")
```

Comments on classical decomposition:

-   Estimate of trend is **unavailable** for first few and last few observations

-   **Seasonal component repeats** from year to year. May not be realistic

-   **Not robust to outliers**

-   Newer methods designed to overcome these problems.

## 3.5 Methods used by official statistics agencies

History of time series decomposition

-   Classical methods originated in the 1920s

-   Census II method introduced in 1957. Basis for X-11 method and variants (including X-12-ARIMA, X-13-ARIMA)

-   STL method introduced in 1983

-   TRAMO/SEATS introduced in the 1990s

National statistics offices

-   ABS uses X-12-ARIMA

-   US Census Bureau uses X-13-ARIMA-SEATS

-   Statistics Canada uses X-12-ARIMA

-   ONS (UK) uses X-12-ARIMA

-   EuroStat uses X-13-ARIMA-SEATS

**X-11 decomposition**

```{r}
x11_dcmp <- us_retail_employment |> 
  model(x11 = X_13ARIMA_SEATS(Employed ~ x11())) |> 
  components()

autoplot(x11_dcmp)
```

Advantages:

-   Relatively robust to outliers

-   Completely automated choices for trend ans seasonal changes

-   Very widely tested on economic data over a long period of time

Disadvantages:

-   No prediction/confidence intervals

-   Ad hoc method with no underlying model

-   Only developed for quarterly and monthly data

Extensions: X-12-ARIMA and X-13-ARIMA

-   The X-11, X-12-ARIMA and X-13-ARIMA methods are based on Census II decomposition

-   These allow adjustments for trading days and other explanatory variables

-   Known outliers can be omitted

-   Level shifts and ramp effects can be modelled

-   Missing values estimated and replaced

-   Holiday factors (e.g. Easter, Labour Day) can be estimated

**X-13-ARIMA-SEATS\
**

```{r}
seats_dcmp <- us_retail_employment |> 
  model(seats = X_13ARIMA_SEATS(Employed ~ seats())) |> 
  components()

autoplot(seats_dcmp)
```

Advantages:

-   Model-based

-   Smooth trend estimate

-   Allows estimated at end points

-   Allows changing seasonality

-   Developed for economic data

Disadvantages:

-   Only developed for quarterly and monthly data

## 3.6 STL decomposition

-   STL: "Seasonal and Trend decomposition using Loess"

-   Very versatile and robust

-   Unlike X-12-ARIMA, STL will handle any type of seasonality

-   Seasonal component allowed to change over time, and rate of change controlled by user

-   Smoothness of trend-cycle also controlled by user

-   Robust to outliers

-   Not trading day or calendar adjustments

-   Only additive

-   Take logs to get multiplicative decomposition

-   Use Box-Cox transformations to get other decompositions

```{r}
us_retail_employment |> 
  model(STL(Employed)) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = "periodic"))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 49))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 19))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 5))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 15) + trend(window = 5))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 15) + trend(window = 6665))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 15) + trend(window = 15))) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 15) + trend(window = 15),
            robust = TRUE)) |> 
  components() |> 
  autoplot() +
  labs(title = "STL decomposition: US retail employment")
```

```{r}
us_retail_employment |> 
  model(STL(Employed ~ season(window = 5))) |> 
  components()
  
us_retail_employment |> 
  model(STL(
    Employed ~ trend(window = 15) + 
      season(window = "periodic"),
    robust = TRUE
  )) |> 
  components()
```

-   `trend(window = ?)` controls wiggliness of trend component

-   `season(window = ?)` controls variation on seasonal component

-   `season(window = 'periodic')` is equivalent to an infinite window

```{r}
us_retail_employment |> 
  model(STL(Employed)) |> 
  components() |> 
  autoplot()
```

-   `STL()` chooses `season(window=13)` by default

-   Can include transformations

**STL decomposition**

-   Algorithm that updates trend and seasonal components iteratively

-   Starts with $\hat{T_t}=0$

-   Uses a mixture of loess and moving averages to successively refine the trend and seasonal estimates

-   The trend window controls loess bandwidth applied to deseasonalised values

-   The season window controls loess bandwidth applied to detrended subseries

-   Robustness weights based on remainder

-   Default season: `window=13`

-   Default trend: `window=nextodd(ceiling((1.5*period)/(1-(1.5/s.window)))`

# Chapter 4 - Time series features

## 4.1 Some simple statistics

```{r}
tourism |> 
  features(Trips, list(mean = mean)) |> 
  arrange(mean)
```

```{r}
tourism |> 
  features(Trips, quantile)
```

## 4.2 ACF features

-   The sum of the first ten squared autocorrelation coefficients is a useful summary of how much autocorrelation there is in a series, regardless of lag.

-   We can also compute autocorrelations of the changes in the series between periods: we "difference" the data and create a new time series consisting of the differences between consecutive observations. Then we can compute the autocorrelations of this new differenced series.

The `feat_acf()` function computes a selection of autocorrelations:

-   the first autocorrelation coefficient from the original data

-   the sum of squares of the first ten autocorrelation coefficients from the original data

-   the first autocorrelation coefficient from the differenced data

-   the sum of squares of the first ten autocorrelation coefficients from the differenced data

-   the first autocorrelation coefficient from the twice differenced data

-   the sum of squares of the first ten autocorrelation coefficients from the twice differenced data

-   for seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned

```{r}
tourism |> features(Trips, feat_acf)
```

## 4.3 STL features

The decomposition is written as

$$
y_t=T_t+S_t+R_t
$$

For strongly trended data, the seasonally adjusted data should have much more variation than the remainder component. Therefore, $Var(R_t)/(Var(T_t+S_t)$ should be relatively small.

But for data with little or no trend, the two variances should be approximately the same. So we define the strength of trend as:

$$
F_T=max(0,1-\frac{Var(R_t)}{Var(T_t+S_t})
$$

The strength of seasonality is defined similarly, but with respect to the detrended data rather than the seasonally adjusted data:

$$
F_S=max(0,1-\frac{Var(R_t)}{Var(S_t+R_t}
$$

```{r}
tourism |> 
  features(Trips, feat_stl)
```

```{r}
tourism |>
  features(Trips, feat_stl) |>
  ggplot(aes(x = trend_strength, y = seasonal_strength_year,
             col = Purpose)) +
  geom_point() +
  facet_wrap(vars(State))
```

```{r}
tourism |>
  features(Trips, feat_stl) |>
  filter(
    seasonal_strength_year == max(seasonal_strength_year)
  ) |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(vars(State, Region, Purpose))
```

## 4.4 Other features

In the `feasts` package: `coef_hurst`, `feat_spectral`, `box_pierce`, ...

## 4.5 Exploring Australian tourism data

All of the features included in the `feasts` package can be computed in one line:

```{r}
tourism_features <- tourism |> 
  features(Trips, feature_set(pkgs = "feasts"))
tourism_features
```

This gives 48 features for every combination of the three key variables (`Region`, `State` and `Purpose`).

We can do pairwise plots of groups of features:

```{r}
library(glue)
tourism_features |>
  select_at(vars(contains("season"), Purpose)) |>
  mutate(
    seasonal_peak_year = seasonal_peak_year +
      4*(seasonal_peak_year==0),
    seasonal_trough_year = seasonal_trough_year +
      4*(seasonal_trough_year==0),
    seasonal_peak_year = glue("Q{seasonal_peak_year}"),
    seasonal_trough_year = glue("Q{seasonal_trough_year}"),
  ) |>
  GGally::ggpairs(mapping = aes(colour = Purpose))
```

We can compute the principal components of the tourism features:

```{r}
library(broom)
pcs <- tourism_features |>
  select(-State, -Region, -Purpose) |>
  prcomp(scale = TRUE) |>
  augment(tourism_features)
pcs |>
  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = Purpose)) +
  geom_point() +
  theme(aspect.ratio = 1)
```

```{r}
outliers <- pcs |>
  filter(.fittedPC1 > 10) |>
  select(Region, State, Purpose, .fittedPC1, .fittedPC2)
outliers
#> # A tibble: 4 × 5
#>   Region                 State             Purpose  .fittedPC1 .fittedPC2
#>   <chr>                  <chr>             <chr>         <dbl>      <dbl>
#> 1 Australia's North West Western Australia Business       13.4    -11.3  
#> 2 Australia's South West Western Australia Holiday        10.9      0.880
#> 3 Melbourne              Victoria          Holiday        12.3    -10.4  
#> 4 South Coast            New South Wales   Holiday        11.9      9.42
outliers |>
  left_join(tourism, by = c("State", "Region", "Purpose"), multiple = "all") |>
  mutate(Series = glue("{State}", "{Region}", "{Purpose}", .sep = "\n\n")) |>
  ggplot(aes(x = Quarter, y = Trips)) +
  geom_line() +
  facet_grid(Series ~ ., scales = "free") +
  labs(title = "Outlying time series in PC space")
```

# Chapter 5 - The forecaster's toolbox

## 5.1 A tidy forecasting workflow

The process of producing forecasts can be split up into a few fundamental steps:

1.  Preparing data - generate a `tsibble`
2.  Data visualisation
3.  Specifying a model
4.  Model estimation - `model()` -\> `mable`
5.  Accuracy & performance evaluation
6.  Producing forecasts - `forecast()` -\> `fable`

![](images/clipboard-1016832891.png)

**Data preparation (tidy)**

```{r}
gdppc <- global_economy |> 
  mutate(GDP_per_capita = GDP / Population) |> 
  select(Year, Country, GDP, Population, GDP_per_capita)
gdppc
```

**Visualise**

```{r}
gdppc |> 
  filter(Country == "Sweden") |> 
  autoplot(GDP_per_capita) +
  labs(title = "GDP per capita for Sweden",
       y = "$US")
```

**Specify a model**

```{r}
fit <- gdppc |> 
  model(trend_model = TSLM(GDP_per_capita ~ trend()))
fit
```

A `mable` is a model table, each cell corresponds to a fitted model.

**Evaluate the model**

```{r}
fit |> 
  forecast(h = "3 years")
```

A `fable` is a forecast table with point forecasts and distributions.

**Visualising forecasts**

```{r}
fit |> 
  forecast(h = "3 years") |> 
  filter(Country == "Sweden") |> 
  autoplot(gdppc) +
  labs(title = "GDP per capita for Sweden",
       y = "$US")
```

## 5.2 Some simple forecasting methods

```{r}
bricks <- aus_production |> 
  filter_index("1970 Q1" ~ "2004 Q1") |> 
  select(Bricks)
```

**Mean method**

-   Forecast of all future values is equal to mean of historical data $\{y_t,...,y_T\}$

-   Forecasts: $\hat{y}_{T+h|T}=\bar{y}=(y_1+...+y_T)/T$

```{r}
bricks |> 
  model(MEAN(Bricks))
```

```{r}
bricks <- aus_production |>
  filter(!is.na(Bricks)) |>
  mutate(average = mean(Bricks))

fc <- bricks |>
  filter(row_number() == n()) |>
  as_tibble() |>
  unnest(Quarter = list(as.Date(Quarter) + months(c(0, 12 * 5))))

bricks |>
  ggplot(aes(x = Quarter, y = Bricks)) +
  geom_line() +
  geom_line(aes(y = average), colour = "#0072B2", linetype = "dashed") +
  geom_line(aes(y = average), data = fc, colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

**Naïve method**

-   Forecasts equal to last observed value

-   Forecasts: $\hat{y}_{T+h|T}=y_T$

-   Consequence of efficient market hypothesis

```{r}
bricks |> model(NAIVE(Bricks))
```

```{r}
bricks |>
  filter(!is.na(Bricks)) |>
  model(NAIVE(Bricks)) |>
  forecast(h = "5 years") |>
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, n()), aes(y = Bricks), colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

**Seasonal naïve method**

-   Forecasts equal to last value from same season

-   Forecasts: $\hat{y}_{T+h|T}=y_{T+h-m(k+1)}$ where $m$ = seasonal period and $k$ is the integer part of $(h-1)/m$

```{r}
bricks |> model(SNAIVE(Bricks ~ lag("year")))
```

```{r}
bricks |>
  model(SNAIVE(Bricks ~ lag("year"))) |>
  forecast(h = "5 years") |>
  autoplot(filter(bricks, year(Quarter) > 1990), level = NULL) +
  geom_point(data = slice(bricks, (n() - 3):n()), aes(y = Bricks), colour = "#0072B2") +
  labs(title = "Clay brick production in Australia")
```

**RW: drift method**

RW: Random Walk

-   Forecasts equal to last value plus average change

-   Forecasts:

$$
\hat{y}_{T+h|T}=y_T+\frac{h}{T-1}\sum_{t=2}^T(y_t-y_{t-1})=y_T+\frac{h}{T-1}(y_T-y_1)
$$

-   Equivalent to extrapolating a line drawn between first and last observations

```{r}
bricks |> model(RW(Bricks ~ drift()))
```

```{r}
aus_production |>
  filter(!is.na(Bricks)) |>
  model(RW(Bricks ~ drift())) |>
  forecast(h = "5 years") |>
  autoplot(aus_production, level = NULL) +
  geom_line(
    data = slice(aus_production, range(cumsum(!is.na(Bricks)))),
    aes(y = Bricks), linetype = "dashed", colour = "#0072B2"
  ) +
  labs(title = "Clay brick production in Australia")
```

**Model fitting**

The `model()` function trains models to data.

```{r}
brick_fit <- aus_production |> 
  filter(!is.na(Bricks)) |> 
  model(
    Seasonal_naive = SNAIVE(Bricks),
    Naive = NAIVE(Bricks),
    Drift = RW(Bricks ~ drift()),
    Mean = MEAN(Bricks)
  )
```

```{r}
brick_fit
```

**Producing forecasts**

```{r}
brick_fc <- brick_fit |> 
  forecast(h = "5 years")
```

```{r}
brick_fc
```

**Visualising forecasts**

```{r}
brick_fc |> 
  autoplot(aus_production, level = NULL) +
  labs(title = "Clay brick production in Australia",
       y = "Millions of bricks") +
  guides(colour = guide_legend(title = "Forecast"))
  
```

**Facebook closing stock price**

```{r}
# Extract training data
fb_stock <- gafa_stock |> 
  filter(Symbol == "FB") |> 
  mutate(trading_day = row_number()) |> 
  update_tsibble(index = trading_day, regular = TRUE)

# Specify, estimate and forecast
fb_stock |> 
  model(
    Mean = MEAN(Close),
    Naive = NAIVE(Close),
    Drift = RW(Close ~ drift())
  ) |> 
  forecast(h = 42) |> 
  autoplot(fb_stock, level = NULL) +
  labs(title = "Facebook closing stock price",
       y = "$US") +
  guides(colour = guide_legend(title = "Forecast"))
```

**Australian quarterly beer production**

```{r}
# Set training data from 1992 to 2006
train <- aus_production |>
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit |> forecast(h = 14)
# Plot forecasts against actual values
beer_fc |>
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

## 5.3 Fitted values and residuals

-   $\hat{y}_{t|t-1}$ is the forecast of $y_t$ based on observations $y_1,...,y_{t-1}$

-   we call these "fitted values"

-   sometimes drop the subscript: $\hat{y_t} \equiv \hat{y}_{t|t-1}$

-   often not true forecasts since parameters are estimated on all data

For example:

-   $\hat{y}_t=\bar{t}$ for average method

-   $\hat{y}=y_{t-1}+(y_T-y_1)/(T-1)$ for drift method

**Residuals in forecasting**: difference between observed value and its fitted value $e_t=y_t-\hat{y}_{t|t-1}$.

Assumptions:

-   $\{e_t\}$ uncorrelated. If they aren't, then information left in residuals that should be used in computing forecasts.

-   $\{e_t\}$ have mean zero. If they don't, then forecasts are biased.

Useful properties (for distributions & prediction intervals

-   $\{e_t\}$ have constant variance.

-   $\{e_t\}$ are normally distributed.

To compute the fitted values and residuals:

```{r}
augment(beer_fit)
```

## 5.4 Residual diagnostics

Facebook closing stock price

```{r}
fb_stock |> 
  autoplot(Close)
```

```{r}
fit <- fb_stock |> 
  model(NAIVE(Close))
augment(fit)
```

```{r}
augment(fit) |> 
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

```{r}
augment(fit) |> 
  filter(trading_day > 1100) |> 
  ggplot(aes(x = trading_day)) +
  geom_line(aes(y = Close, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted"))
```

```{r}
augment(fit) |> 
  autoplot(.resid) +
  labs(y = "$US",
       title = "Residuals from naïve method")
```

```{r}
augment(fit) |> 
  ggplot(aes(x = .resid)) +
  geom_histogram(bins = 150) +
  labs(title = "Histogram of residuals")
```

```{r}
augment(fit) |> 
  ACF(.resid) |> 
  autoplot() +
  labs(title = "ACF of residuals")
```

```{r}
gg_tsresiduals(fit)
```

ACF of residuals

-   We assume that the residuals are white noise (uncorrelated, mean zero, constant variance). If they aren't, then there is information left in the residuals that should be used in computing forecasts.

-   So a standard residual diagnostic is to check the ACF of the residuals of a forecasting method.

-   We *expect* these to look like white noise.

**Portmanteau tests**

$r_k$=autocorrelation of residual at lag $k$

Consider a whole set of $r_k$ values, and develop a test to see whether the set is significantly different from a zero test.

Box-Pierce test :

$$
Q=T\sum_{k=1}^lr_k^2
$$

where $l$ is max lag being considered and $T$ is number of observations.

-   if each $r_k$ close to 0, $Q$ will be small

-   if some $r_k$ values large (positive or negative), $Q$ will be large

Ljung-Box test :

$$
Q^*=T(T+2)\sum_{k=1}^l(T-k)^{-1}r_k^2
$$

where $k$ is max lag being considered and $T$ is number of observations.

-   my preferences: $l$=10 for non-seasonal data, $l$=2m for seasonal data (where $m$ is seasonal period)

-   better performance, especially in small samples

-   if data were white noise, $Q^*$ has a $\chi^2$ distribution with $l$ degrees of freedom

-   lag = $l$

```{r}
augment(fit) |> 
  features(.resid, ljung_box, lag = 10)
```

If pval \< 0.05, we reject the white noise hypothesis.

In our case, we accept the hypothesis that the residuals are not easily distinguished from white noise.

## 5.5 Distributional forecasts and prediction intervals

-   A forecast $\hat{y}_{T+h|T}$ is (usually) the mean of the conditional distribution $y_{T+h}|y_1,...,y_T$.

-   Most time series models produce normally distributed forecasts.

-   The forecast distribution describes the probability of observing any future value.

Assuming residuals are normal, uncorrelated, $sd=\hat{\sigma}$:

**Mean**: $y_{T+h|T} \sim N(\hat{y},(1+1/T)\hat{\sigma}^2)$

**Naïve**: $y_{T+h|T} \sim N(y_T,h\hat{\sigma}^2)$

**Seasonal naîve**: $y_{T+h|T} \sim N(y_{T+h-m(k+1)},(k+1)\hat{\sigma}^2)$

**Drift**: $y_{T+h|T} \sim N(y_T+\frac{h}{T-1}(y_T-y_1),h\frac{T+h}{T}\hat{\sigma}^2)$

where $k$ is the integer part of $(h-1)/m$.

Note that when $h=1$ and $T$ is large, these all give the same approximate forecast variance: $\hat{\sigma}^2$.

-   A prediction interval gives a region within which we expect $y_{T+h}$ to lie with a specified probability

-   Assuming forecast errors are normally distributed, then a 95% PI is $\hat{y}_{T+h|T} \pm 1.96\hat{\sigma}_h$ where $\hat{\sigma}_h$ is the st dev of the h-step distribution

-   When h=1, $\hat{\sigma}_h$ can be estimated from the residuals

```{r}
aus_production |> 
  filter(!is.na(Bricks)) |> 
  model(Seasonal_naive = SNAIVE(Bricks)) |> 
  forecast(h = "5 years")
```

```{r}
aus_production |> 
  filter(!is.na(Bricks)) |> 
  model(Seasonal_naive = SNAIVE(Bricks)) |> 
  forecast(h = "5 years") |> 
  hilo(level = 95)
```

```{r}
aus_production |> 
  filter(!is.na(Bricks)) |> 
  model(Seasonal_naive = SNAIVE(Bricks)) |> 
  forecast(h = "5 years") |> 
  hilo(level = 95) |> 
  mutate(lower = `95%`$lower,
         upper = `95%`$upper)
```

-   Point forecasts are often useless without a measure of uncertainty (such as prediction intervals)

-   Prediction intervals require a stochastic model (with random errors, etc.)

-   For most models, prediction intervals get wider as the forecast horizon increases

-   Use `level` argument to control coverage

-   Check residual assumptions before believing them

-   Prediction intervals are usually too narrow due to unaccounted uncertainty

## 5.6 Forecasting using transformations

If the data show different variations at different levels of the series, then a transformation can be useful.

Denote original observations as $y_1, ..., y_n$ and transformed observations as $w_1, ..., w_n$.

Box-Cox transformations:

$$
w_t = \left\{\begin{array}{ll}
        \log(y_t),      & \quad \lambda = 0; \\
        (sign(y_t)|y_t|^\lambda-1)/\lambda ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

-   Natural logarithm, particularly useful because they are more interpretable: changes in a log value are relative (percent) changes on the original scale.

We must reverse the transformation or back-transform to obtain forecasts on the original scale. The reverse Box-Cox transformations are given by:

$$
w_t = \left\{\begin{array}{ll}
        \exp(w_t),      & \quad \lambda = 0; \\
        (sign(\lambda w_t+1)|\lambda w_t+1|^{1 /\lambda}, & \quad \lambda \ne 0.
\end{array}\right.
$$

```{r}
eggs <- prices |> 
  filter(!is.na(eggs)) |> 
  select(eggs)
eggs |> 
  autoplot() +
  labs(title = "Annual egg prices",
       y = "$US (adjusted for inflation)")
```

Transformations used in the left of the formula will be automatically back-transformed. To model log-transformed egg prices, you could use:

```{r}
fit <- eggs |> 
  model(RW(log(eggs) ~ drift()))
fit
```

```{r}
fc <- fit |> 
  forecast(h = 50)
fc
```

```{r}
fc |> 
  autoplot(eggs) +
  labs(title = "Annual egg prices",
       y = "US$ (adjusted for inflation)")
```

-   Back-transformed point forecasts are medians

-   Back-transformed PI have the correct coverage

**Back-transformed means**

Let $X$ have mean $\mu$ and variance $\sigma^2$

Let $f(x)$ be back-transformed function, and $Y=f(X)$

Taylor series expansion about $\mu$:

$$
Y=f(X) \approx f(\mu) + (X-\mu)f'(\mu)+\frac{1}{2}(X-\mu)^2f''(\mu)
$$

$$
E[Y]=E[f(X)] \approx f(\mu)+\frac{1}{2}\sigma^2f''(\mu)
$$

**Bias adjustment**

Box-Cox back-transformation:
$$
y_t = \left\{\begin{array}{ll}
        \exp(w_t),      & \quad \lambda = 0; \\
        (\lambda w_t+1)^{1/\lambda} ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

$$
f(x) = \left\{\begin{array}{ll}
        e^x,      & \quad \lambda = 0; \\
        (\lambda x+1)^{1/\lambda} ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

$$
f''(x) = \left\{\begin{array}{ll}
        e^x,      & \quad \lambda = 0; \\
        (1- \lambda)(\lambda x+1)^{1/{\lambda-2}} ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

$$
E[Y] \approx \left\{\begin{array}{ll}
        e^\mu[1+\frac{\sigma^2}{2}],      & \quad \lambda = 0; \\
        (\lambda\mu+1)^{1/\lambda}[1+\frac{\sigma^2(1- \lambda)}{2(\lambda\mu+1)^2}] ,         & \quad \lambda \ne 0.
\end{array}\right.
$$

```{r}
fc |> 
  autoplot(eggs, level = 80, point_forecast = lst(mean, median)) +
  labs(title = "Annual egg prices",
       y = "$US (adjusted for inflation)")
```

## 5.7 Forecasting with decomposition

$$
y=\hat{S_t}+\hat{A_t}
$$

-   $\hat{A_t}=\hat{T_t}+\hat{R_t}$ is the seasonally adjusted component

-   $\hat{S_t}$ is the seasonal component

-   Forecast $\hat{S_t}$ using SNAIVE

-   Forecast $\hat{A_t}$ using non-seasonal time series method

-   Combine forecasts of $\hat{S_t}$ and $\hat{A_t}$ to get forecasts of original data

```{r}
us_retail_employment <- us_employment |> 
  filter(year(Month) >= 1990, Title == "Retail Trade") |> 
  select(-Series_ID)
us_retail_employment
```

```{r}
dcmp <- us_retail_employment |> 
  model(STL(Employed)) |> 
  components() |> 
  select(-.model)
dcmp
```

```{r}
dcmp |> 
  model(NAIVE(season_adjust)) |> 
  forecast() |> 
  autoplot(dcmp) +
  labs(title = "Naive forecasts of seasonally adjusted data")
```

```{r}
us_retail_employment |> 
  model(stlf = decomposition_model(
    STL(Employed ~ trend(window = 7), robust = TRUE),
    NAIVE(season_adjust)
  )) |> 
  forecast() |> 
  autoplot(us_retail_employment)
```

`decomposition_model()` creates a decomposition model

-   You must provide a method for forecasting the `season_adjust` series

-   A seasonal naive method is used by default for the `seasonal` components

-   The variances from both the seasonally adjusted and seasonal forecasts are combined

## 5.8 Evaluating point forecast accuracy

**Training and test sets**

-   A model which fits the training data well will not necessarily forecast well

-   A perfect fit can always be obtained by using a model with enough parameters

-   Over-fitting a model to data is just as bad as failing to identify a systematic pattern in the data

-   The test set must not be used for *any* aspect of model development or calculation of forecasts

-   Forecast accuracy is based only on the test set

Forecast "error": the difference between an observed value and its forecast.

$$
e_{T+h}=y_{T+h}-\hat{y_{T+h|T}}
$$

where the training data is given by $\{y_1,...,y_T\}$.

-   Unlike residuals, forecast errors on the test set involve multi-step forecasts.

-   There are *true* forecast errors as the test data is not used in computing $\hat{y}_{T+h|T}$.

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
beer_train <- recent_production |>
  filter(year(Quarter) <= 2007)

beer_fit <- beer_train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )

beer_fc <- beer_fit |>
  forecast(h = 10)

beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

**Measures of forecast accuracy**

$y_{T+h}$: (T+h)th observation, h =1, ... H

$\hat{y}_{T+h}$: its forecast based on data up to time T

$e_{T+h}=y_{T+h}-\hat{y}_{T+h|T}$

ME = mean($e_{T+h})$

MAE = mean($|e_{T+h}|)$

MSE = mean($e^2_{T+h})$

RMSE = sqrt(mean($e^2_{T+h})$)

MAPE = 100mean($|e_{T+h}|/|y_{T+h}|)$

-   MAE, MSE, RMSE are all scale dependent

-   MAPE is scale independent but is only sensible if $y_t >> 0$ for all t, and y has a natural zero.

Proposed by Hyndman and Koehler (IJF, 2006):

-   For non-seasonal time series, scale errors using naïve forecasts

$$
q_j=\frac{e_j}{\frac{1}{T-1}\sum_{t=2}^T|y_t-y_{t-1}|}
$$

-   For seasonal time series, scale forecast errors using seasonal naïve forecasts

$$
q_j=\frac{e_j}{\frac{1}{T-m}\sum_{t=m+1}^T|y_t-y_{t-m}|}
$$

Mean Absolute Scaled Error: MASE = $mean(|q_j|)$

Root Mean Squared Scaled Error: RMSSE = $\sqrt{mean(q_j^2)}$

where

$$
q_j^2=\frac{e^2_j}{\frac{1}{T-m}\sum_{t=2}^T(y_t-y_{t-m})^2}
$$

and we set m=1 for non-seasonal data

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)
beer_train <- recent_production |>
  filter(year(Quarter) <= 2007)

beer_fit <- beer_train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer),
    Drift = RW(Beer ~ drift())
  )

beer_fc <- beer_fit |>
  forecast(h = 10)

beer_fc |>
  autoplot(
    aus_production |> filter(year(Quarter) >= 1992),
    level = NULL
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```

**Measures of forecast accuracy**

```{r}
accuracy(beer_fit) |> 
  arrange(.model) |> 
  select(.model, .type, RMSE, MAE, MAPE, MASE, RMSSE)
```

```{r}
accuracy(beer_fc, recent_production) |> 
  arrange(.model) |> 
  select(.model, .type, RMSE, MAE, MAPE, MASE, RMSSE)
```
